{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the code, as well as the resulting folder structures, is based on https://cs230-stanford.github.io/pytorch-getting-started.html and \n",
    "https://github.com/cs230-stanford/cs230-code-examples/tree/master/pytorch/vision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset Preprocessing\n",
    "##### Required Folder Structure:\n",
    "- data\n",
    " - data/heart_scans *(directory for \"data_dir\")*\n",
    " \n",
    "##### Parameters:\n",
    " - data_dir: (string) The location of the original dataset. May contain subfolders.\n",
    " - output_dir: (string) The location where the preprocessed dataset should be stored.\n",
    " - n4: (bool) Whether N4 Bias Field Correction is applied to the dataset.\n",
    " - roi: (bool) Whether rudimentary ROI is applied to the dataset.\n",
    " - rot: (bool) Whether dataset is augmented by rotating it 3 times in semi-random degrees.\n",
    " - h_flip: (bool) Whether dataset is augmented by horizontally flipping it.\n",
    " - v_flip: (bool) Whether dataset is augmented by vertically flipping it.\n",
    " - scale: (bool) Wheter dataset is augemented by random scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for missing ENDO or EPI ...\n",
      "   Done. Found 0 missing ENDO and 0 missing EPI.\n",
      "Split filenames into train and test set ...\n",
      "    Done.\n",
      "Warning: output dir data/320x320_heart_scans_rot already exists\n",
      "Warning: dir data\\320x320_heart_scans_rot\\train_heart_scans already exists\n",
      "Resizing train data, saving resized data to data\\320x320_heart_scans_rot\\train_heart_scans\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 840/840 [00:10<00:00, 54.68it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting train data, saving augmented data to data\\320x320_heart_scans_rot\\train_heart_scans\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 840/840 [00:32<00:00, 10.58it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: dir data\\320x320_heart_scans_rot\\test_heart_scans already exists\n",
      "Resizing test data, saving resized data to data\\320x320_heart_scans_rot\\test_heart_scans\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171/171 [00:02<00:00, 85.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting test data, saving augmented data to data\\320x320_heart_scans_rot\\test_heart_scans\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171/171 [00:07<00:00, 16.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done building dataset.\n"
     ]
    }
   ],
   "source": [
    "import build_dataset\n",
    "\n",
    "build_dataset.main(data_dir=\"data/heart_scans\", \n",
    "                   output_dir=\"data/{}x{}_heart_scans\".format(build_dataset.IMG_SIZE, build_dataset.IMG_SIZE,), \n",
    "                   n4=False, \n",
    "                   roi=False, \n",
    "                   rot=True, \n",
    "                   h_flip=False, \n",
    "                   v_flip=False, \n",
    "                   scale=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Experiments using k-fold CV (Phase 1)\n",
    "## 2.1. Single Hyperparameter Search\n",
    "##### Required Folder Structure:\n",
    "- data\n",
    " - data/320x320_heart_scans<postfix indicating augmentation, n4, roi\\> *(directory for \"data_dir\")*\n",
    "    - data/320x320_heart_scans<postfix indicating augmentation, n4, roi\\>/train_heart_scans\n",
    "    - data/320x320_heart_scans<postfix indicating augmentation, n4, roi\\>/test_heart_scans\n",
    "- experiments\n",
    " - ... *(directory for \"model_dir\" and \"parent_dir\"; must contain hyper_params.json, as specified below)*\n",
    "\n",
    "##### Required JSON File:\n",
    "Must contain a file named \"hyper_params.json\" with the following content \\[possible values specified after \\>\\]:\n",
    "\n",
    "`{\n",
    "    \"endo_or_epi\": \"EPI\",     > [\"EPI\", \"ENDO\"]\n",
    "    \"model\": \"ModelA\",        > [\"ModelA\", \"UNetOriginal\", \"UNetOriginalPretrained\"]\n",
    "    \"augmentation\": \"_rot\",   > [\"_rot\", \"_rot_flip_scale\", \"n4_rot\", ...]\n",
    "    \"learning_rate\": 0.0004, \n",
    "    \"optimizer\": \"Adam\",      \n",
    "    \"loss\": \"soft_dice_loss\", > [\"soft_dice_loss\", \"binary_cross_entropy_loss\"]\n",
    "    \"activation\": \"ReLU\",     > [\"ReLU\", \"ELU\"]\n",
    "    \"batch_size\": 12,\n",
    "    \"num_epochs\": 45,\n",
    "    \"batch_norm\": 1,          > [0, 1]\n",
    "    \"dropout_rate\": 0.0,\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"treshold\": 0.5,\n",
    "    \"num_workers\": 4\n",
    "}`\n",
    "\n",
    "##### Parameters:\n",
    "- data_dir: (string) The location of the preprocessed dataset, as was specified as \"output_dir\" during \"1. Dataset Preprocessing\", **_not including the postfix indicating the augmentation or application of N4 and ROI_**!\n",
    "- model_dir: (string) Directory containing hyper_params.json and/or, for transfer learning, the saved model as a \\*.pth.tar file.\n",
    "- restore_file: (string) If transfer learning is used, specify name of the \\*.pth.tar file containing the starting weights and biases, omitting \".pth.tar\". Must be located in the location of \"model_dir\". None, for no transfer learning (random weight initialization).\n",
    "- k_folds: (int) Specify the number of folds (k) for k-fold CV. (Setting to 0 to turn off k-fold CV will result in \"3. Final Training of Models (Phase 2)\" taking place.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added new handler!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the datasets...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 300.0052185058594, Std: 430.4715881347656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- done.\n",
      "For k-fold 1/5:\n",
      "Starting training for 45 epoch(s)\n",
      "Epoch 1/45\n",
      "100%|██████████| 75/75 [00:28<00:00,  3.43it/s, loss=0.959]\n",
      "- Train metrics: dsc: 0.274 ; iou: 0.182 ; accuracy: 0.920 ; precision: 0.182 ; recall: 0.928 ; loss: 0.959\n",
      "- Eval metrics : dsc: 0.484 ; iou: 0.345 ; accuracy: 0.985 ; precision: 0.345 ; recall: 0.968 ; loss: 0.950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint Directory exists! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- Found new best DSC\n",
      "Epoch 2/45\n",
      " 51%|█████     | 38/75 [00:15<00:12,  3.07it/s, loss=0.943]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-792192128316>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m            \u001b[0mmodel_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"experiments/ENDO/ModelA_random/batch_size/batch_size_12\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m            \u001b[0mrestore_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m            k_folds=5)\n\u001b[0m",
      "\u001b[1;32mD:\\BachelorThesis\\unet_lv_segmentation\\train.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m(data_dir, model_dir, restore_file, k_folds)\u001b[0m\n\u001b[0;32m    223\u001b[0m             \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"For k-fold {}/{}:\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_folds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Starting training for {} epoch(s)\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhyper_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m             \u001b[1;33m(\u001b[0m\u001b[0mall_train_metrics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_val_metrics\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_dl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhyper_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrestore_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrestore_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m             \u001b[1;31m# Write to Tesorboard.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mepoch_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_train_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\BachelorThesis\\unet_lv_segmentation\\train.py\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(model, train_dataloader, val_dataloader, optimizer, loss_fn, metrics_dict, hyper_params, model_dir, restore_file)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[1;31m# compute number of batches in one epoch (one full pass over the training set)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         \u001b[0mtrain_metrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhyper_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m         \u001b[0mall_train_metrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'epoch_{:02d}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_metrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\BachelorThesis\\unet_lv_segmentation\\train.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, loss_fn, dataloader, metrics_dict, hyper_params)\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;31m#if i % hyper_params.save_summary_steps == 0:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[1;31m# extract data from torch Variable, move to cpu, convert to numpy arrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m             \u001b[0moutput_batch_np\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m             \u001b[0mground_truth_batch_np\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mground_truth_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import train\n",
    "\n",
    "# Example: Perform hyperparameter experiment for endocardium for a batch size of 12.\n",
    "train.main(data_dir=\"data/320x320_heart_scans\", \n",
    "           model_dir=\"experiments/ENDO/ModelA_random/batch_size/batch_size_12\", \n",
    "           restore_file=None, \n",
    "           k_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import train\n",
    "\n",
    "# Same example for Cross Transfer Learning.\n",
    "train.main(data_dir=\"data/320x320_heart_scans\", \n",
    "           model_dir=\"experiments/ENDO/ModelA_pretrained/batch_size/batch_size_12\", \n",
    "           restore_file=\"best_epi_exp_68\", \n",
    "           k_folds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Multiple Hyperparameter Search\n",
    "##### Required Folder Structure:\n",
    "Same as for \"2.1. Single Hyperparameter Search\".\n",
    "##### Required JSON File:\n",
    "Same as for \"2.1. Single Hyperparameter Search\", but requires that an array is specified for exactly one of the hyperparameters, e.g.:\n",
    "\n",
    "`{\n",
    "    ...    \n",
    "    \"learning_rate\": [0.01, 0.001, 0.0001], \n",
    "    ...\n",
    "}`\n",
    "\n",
    "##### Parameters:\n",
    "- data_dir: (string) The location of the preprocessed dataset, as was specified as \"output_dir\" during \"1. Dataset Preprocessing\", **_not including the postfix indicating the augmentations or application of N4 and ROI_**!\n",
    "- parent_dir: (string) Folder containing hyper_params.json, specifying the different values for the hyperparameter to be tried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import search_hyperparams\n",
    "\n",
    "# Example: Perform hyperparameter search for epicardium over various learning rates.\n",
    "search_hyperparams.main(data_dir=\"data/320x320_heart_scans\",\n",
    "                        parent_dir=\"experiments/EPI/learning_rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Numerical Evaluation of Experiments\n",
    "##### Required Folder Structure:\n",
    "- experiments *(directory for \"parent_dir\")*\n",
    " - ... *(subfolders must contain various JSON files, as specified below)*\n",
    " \n",
    "##### Required JSON Files:\n",
    "All the JSON files that were generated by \"2. Experiments using k-fold CV (Phase 1)\", e.g. \"metrics_k_fold_train.json\" or \"metrics_k_fold_val_average_best.json\". Must be located in the directory specified for \"parent_dir\" or in any of its subfolders.\n",
    "\n",
    "##### Parameters:\n",
    "- parent_dir: (string) Folder containing (subfolders with) the JSON files created by all experiments run for \"2. Experiments using k-fold CV (Phase 1)\". The file \"results.md\", containing the tabular overview generated by running \"synthesize_results\", will be located here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import synthesize_results\n",
    "\n",
    "synthesize_results.main(parent_dir=\"experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Graphical Evaluation of Experiments\n",
    "For a graphical evaluation of experiments, start a console at the same directory containing this file and run \"tensorboard --logdir=tensor_log\\\\_<specifiy subfolder structure for the graphs you want to evaluate\\>_\", then follow instructions in the console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Final Training of Models (Phase 2)\n",
    "##### Required Folder Structure:\n",
    "Same as for \"2. Experiments using k-fold CV (Phase 1)\".\n",
    "##### Required JSON File:\n",
    "Same as for \"2.1. Single Hyperparameter Search\".\n",
    "##### Parameters:\n",
    "- data_dir: (string) The location of the preprocessed dataset, as was specified as \"output_dir\" during \"1. Dataset Preprocessing\", **_not including the postfix indicating the augmentation or application of N4 and ROI_**!\n",
    "- model_dir: (string) Folder containing hyper_params.json and/or, for transfer learning, the saved model as a \\*.pth.tar file.\n",
    "- restore_file: (string) If transfer learning is used, specify name of the \\*.pth.tar file containing the starting weights and biases, omitting \".pth.tar\". Must be located in the location of \"model_dir\". None, for no transfer learning (random weight initialization).\n",
    "- k_folds: (int) Set to 0 to turn off k-fold CV for this phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import train\n",
    "\n",
    "# Example: Final Training for endocardium, with batch size 12.\n",
    "train.main(data_dir=\"data/320x320_heart_scans\", \n",
    "           model_dir=\"experiments/ENDO/ModelA_random/batch_size/batch_size_12\", \n",
    "           restore_file=None, \n",
    "           k_folds=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Optional: Run Numerical Evaluation again (see 2.3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import synthesize_results\n",
    "\n",
    "synthesize_results.main(parent_dir=\"experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Segmenting LV Images\n",
    "##### Required Folder Structure:\n",
    "- data\n",
    " - data/320x320_heart_scans<postfix indicating augmentation, n4, roi\\> *(directory for \"data_dir\")*\n",
    "    - data/320x320_heart_scans<postfix indicating augmentation, n4, roi\\>/test_heart_scans *(containing the preprocessed images to be segmented)*\n",
    "- experiments\n",
    " - ... *(directory for \"model_dir\"; must contain hyper_params.json, as specified below; must contain *.pth.tar file containing the trained model)*\n",
    "\n",
    "##### Required JSON File:\n",
    "- Same structure as for \"2.1. Single Hyperparameter Search\", with additional entries for \"mean\" and \"bias\".\n",
    "- MUST be the one modified by \"3. Final Training of Models (Phase 2)\", so that it contains the correct mean and bias of the training set the model was trained with.\n",
    "\n",
    "##### Parameters:\n",
    "- data_dir: (string) The location of the preprocessed images to be segmented, as was specified as \"output_dir\" during \"1. Dataset Preprocessing\", **_not including the postfix indicating the augmentation or application of N4 and ROI_**!\n",
    "- model_dir: (string) Folder containing hyper_params.json and the saved model as a \\*.pth.tar file.\n",
    "- restore_file: (string) Specify name of the \\*.pth.tar file containing the model trained in \"3. Final Training of Models (Phase 2)\", omitting \".pth.tar\". Must be located in the location of \"model_dir\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# Endocardium Segmentation using the model proposed as best, from Experiment 35 (see Thesis).\n",
    "evaluate.main(data_dir=\"data/320x320_heart_scans\", \n",
    "              model_dir=\"experiments/ENDO/best_exp_no_35\", \n",
    "              restore_file=\"best_endo_exp_35\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# Epicardium Segmentation using the model proposed as best, from Experiment 68 (see Thesis).\n",
    "evaluate.main(data_dir=\"data/320x320_heart_scans\", \n",
    "              model_dir=\"experiments/EPI/best_exp_no_68\", \n",
    "              restore_file=\"best_epi_exp_68\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Optional: Run Numerical Evaluation again (see 2.3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import synthesize_results\n",
    "\n",
    "synthesize_results.main(parent_dir=\"experiments\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Optional: Numerical Results per Image\n",
    "\n",
    "Look at \"metrics_test_single_file_names.json\" for the numerical results for each image, sorted from best to worst.\n",
    "\n",
    "Example:\n",
    "\n",
    "`{\n",
    "    \"LAED-BL-S_CineMR_ti00_sl07_EPI_AUG2.png\": {\n",
    "        \"dsc\": 0.9869700074195862,\n",
    "        \"iou\": 0.9742752313613892,\n",
    "        \"accuracy\": 0.999384765625,\n",
    "        \"precision\": 0.9843234419822693,\n",
    "        \"recall\": 0.9896308779716492,\n",
    "        \"loss\": 0.01472163200378418\n",
    "    },\n",
    "    \"BATH-BLn2_CineMR_ti00_sl06_EPI.png\": {\n",
    "        \"dsc\": 0.9865750670433044,\n",
    "        \"iou\": 0.9735058546066284,\n",
    "        \"accuracy\": 0.99916015625,\n",
    "        \"precision\": 0.9884266257286072,\n",
    "        \"recall\": 0.9847304224967957,\n",
    "        \"loss\": 0.01432490348815918\n",
    "    },\n",
    "    \"GOMA-BL-S_CineMR_ti00_sl06_EPI_AUG2.png\": {\n",
    "        \"dsc\": 0.9836491942405701,\n",
    "        \"iou\": 0.9678245186805725,\n",
    "        ...\n",
    "    },\n",
    "  ...\n",
    "}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
